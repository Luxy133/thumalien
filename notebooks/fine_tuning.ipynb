{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning du Classifieur Fake News\n",
    "\n",
    "Ce notebook guide le fine-tuning du modele RoBERTa pour la detection de fake news.\n",
    "\n",
    "**Etapes :**\n",
    "1. Chargement et exploration du dataset\n",
    "2. Fine-tuning du modele\n",
    "3. Evaluation detaillee\n",
    "4. Comparaison avant/apres fine-tuning\n",
    "5. Test sur des exemples reels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "\n",
    "sns.set_theme(style='whitegrid')\n",
    "print('Setup OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Chargement et exploration du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.training.dataset import (\n",
    "    load_liar_dataset, load_fake_news_kaggle, merge_datasets,\n",
    "    get_dataset_stats, ID2LABEL, LABEL2ID\n",
    ")\n",
    "\n",
    "# Option A : LIAR seul (recommande pour commencer)\n",
    "dataset = load_liar_dataset()\n",
    "\n",
    "# Option B : LIAR + Kaggle combines (decommenter pour plus de donnees)\n",
    "# liar = load_liar_dataset()\n",
    "# kaggle = load_fake_news_kaggle()\n",
    "# dataset = merge_datasets(liar, kaggle)\n",
    "\n",
    "# Option C : Dataset personnalise\n",
    "# from src.training.dataset import load_custom_csv\n",
    "# dataset = load_custom_csv('data/raw/mon_dataset.csv')\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistiques du dataset\n",
    "stats = get_dataset_stats(dataset)\n",
    "for split, info in stats.items():\n",
    "    print(f'\\n=== {split.upper()} ({info[\"total\"]} exemples) ===')\n",
    "    for label, data in info['distribution'].items():\n",
    "        bar = '█' * int(float(data['pct'].replace('%', '')) / 2)\n",
    "        print(f'  {label:>10s} : {data[\"count\"]:5d} ({data[\"pct\"]:>6s}) {bar}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution des labels\n",
    "train_labels = [ID2LABEL[l] for l in dataset['train']['label']]\n",
    "fig = px.histogram(x=train_labels, color=train_labels,\n",
    "                   color_discrete_map={'fiable': '#2ecc71', 'douteux': '#f39c12', 'fake': '#e74c3c'},\n",
    "                   title='Distribution des labels (train set)')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Longueur des textes\n",
    "train_texts = dataset['train']['text']\n",
    "lengths = [len(t.split()) for t in train_texts]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.hist(lengths, bins=50, edgecolor='black', alpha=0.7)\n",
    "ax.axvline(np.mean(lengths), color='red', linestyle='--', label=f'Moyenne: {np.mean(lengths):.0f} mots')\n",
    "ax.axvline(np.median(lengths), color='blue', linestyle='--', label=f'Mediane: {np.median(lengths):.0f} mots')\n",
    "ax.set_xlabel('Nombre de mots')\n",
    "ax.set_ylabel('Frequence')\n",
    "ax.set_title('Distribution de la longueur des textes (train)')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'Min: {min(lengths)}, Max: {max(lengths)}, Moyenne: {np.mean(lengths):.1f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemples par label\n",
    "for label_id, label_name in ID2LABEL.items():\n",
    "    examples = [t for t, l in zip(train_texts, dataset['train']['label']) if l == label_id][:3]\n",
    "    print(f'\\n=== {label_name.upper()} ===')\n",
    "    for ex in examples:\n",
    "        print(f'  - {ex[:120]}...' if len(ex) > 120 else f'  - {ex}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.training.train import train\n",
    "\n",
    "# Lancer le fine-tuning\n",
    "# Ajuster les hyperparametres selon vos ressources :\n",
    "#   - GPU : epochs=3, batch_size=16\n",
    "#   - CPU : epochs=2, batch_size=8, max_length=128\n",
    "\n",
    "metrics = train(\n",
    "    model_name='roberta-base',       # ou 'camembert-base' pour le francais\n",
    "    dataset_name='liar',             # 'liar', 'kaggle', 'liar+kaggle', 'custom'\n",
    "    output_dir='../data/models/fake_news_detector',\n",
    "    epochs=3,\n",
    "    batch_size=16,\n",
    "    learning_rate=2e-5,\n",
    "    max_length=256,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher les metriques d'entrainement\n",
    "print('=== Metriques finales ===')\n",
    "for key, value in metrics['test_metrics'].items():\n",
    "    if isinstance(value, float):\n",
    "        print(f'  {key:25s} : {value:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Evaluation detaillee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.training.evaluate import evaluate\n",
    "\n",
    "report = evaluate(\n",
    "    model_path='../data/models/fake_news_detector',\n",
    "    dataset_name='liar',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metriques par classe\n",
    "per_class = pd.DataFrame(report['per_class']).T\n",
    "per_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher la matrice de confusion generee\n",
    "from IPython.display import Image\n",
    "Image('../data/models/fake_news_detector/confusion_matrix.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher la distribution de confiance\n",
    "Image('../data/models/fake_news_detector/confidence_distribution.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Comparaison avant/apres fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.fake_news_detector import FakeNewsDetector\n",
    "\n",
    "# Charger le modele fine-tune (charge automatiquement depuis data/models/)\n",
    "detector_ft = FakeNewsDetector()\n",
    "print(f'Modele fine-tune : {detector_ft.is_finetuned}')\n",
    "\n",
    "# Charger le modele de base pour comparaison\n",
    "detector_base = FakeNewsDetector.__new__(FakeNewsDetector)\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "detector_base.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "detector_base.tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
    "detector_base.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    'roberta-base', num_labels=3\n",
    ").to(detector_base.device)\n",
    "detector_base.model.eval()\n",
    "detector_base.is_finetuned = False\n",
    "print('Modele de base charge pour comparaison')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparer sur des exemples\n",
    "test_texts = [\n",
    "    \"The president announced new economic measures at the press conference.\",\n",
    "    \"BREAKING: Aliens have landed in Paris, the government is hiding the truth!!!\",\n",
    "    \"According to WHO, the vaccine is safe and effective after phase 3 clinical trials.\",\n",
    "    \"They're hiding everything! The elites control the world with 5G, wake up!\",\n",
    "    \"The football match ended with a score of 2-1.\",\n",
    "    \"Scientists have discovered that drinking bleach cures all diseases.\",\n",
    "    \"The unemployment rate decreased by 0.5% this quarter according to official statistics.\",\n",
    "]\n",
    "\n",
    "print(f'{\"Texte\":<60s} | {\"Base\":>12s} | {\"Fine-tune\":>12s}')\n",
    "print('-' * 92)\n",
    "\n",
    "for text in test_texts:\n",
    "    pred_base = detector_base.predict(text)\n",
    "    pred_ft = detector_ft.predict(text)\n",
    "    \n",
    "    base_str = f'{pred_base[\"label\"]} ({pred_base[\"confidence\"]:.0%})'\n",
    "    ft_str = f'{pred_ft[\"label\"]} ({pred_ft[\"confidence\"]:.0%})'\n",
    "    \n",
    "    print(f'{text[:58]:<60s} | {base_str:>12s} | {ft_str:>12s}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Test sur des posts Bluesky reels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from src.collector.bluesky_client import BlueskyCollector\n",
    "from src.preprocessing.text_processor import preprocess_batch\n",
    "\n",
    "load_dotenv('../.env')\n",
    "\n",
    "collector = BlueskyCollector(os.getenv('BLUESKY_HANDLE'), os.getenv('BLUESKY_PASSWORD'))\n",
    "raw_posts = collector.search_posts('fake news', lang='fr', limit=20)\n",
    "processed = preprocess_batch(raw_posts)\n",
    "\n",
    "print(f'{len(processed)} posts collectes et pretraites')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification avec le modele fine-tune\n",
    "for post in processed[:10]:\n",
    "    pred = detector_ft.predict(post['clean_text'])\n",
    "    emoji = {'fiable': '✅', 'douteux': '⚠️', 'fake': '❌'}[pred['label']]\n",
    "    print(f'{emoji} [{pred[\"label\"]:>7s}] ({pred[\"confidence\"]:.0%}) @{post[\"author_handle\"]}')\n",
    "    print(f'   {post[\"text\"][:100]}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Conclusion\n",
    "\n",
    "Le fine-tuning ameliore significativement les performances du modele :\n",
    "- Le modele de base attribue des scores quasi-aleatoires (pas entraine pour cette tache)\n",
    "- Le modele fine-tune distingue les 3 classes avec un F1-score mesurable\n",
    "\n",
    "**Pistes d'amelioration :**\n",
    "- Utiliser `camembert-base` pour de meilleures performances en francais\n",
    "- Augmenter le dataset avec des donnees françaises annotees\n",
    "- Combiner LIAR + Kaggle pour plus de diversite\n",
    "- Experimenter avec des learning rates plus bas (1e-5) et plus d'epoques"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
